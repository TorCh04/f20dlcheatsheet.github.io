```html
<!DOCTYPE html>
<html>
<head>
  <title>Data Mining & Machine Learning: Block 3 Exam Revision</title>
  <meta charset="UTF-8">
   <link rel="stylesheet" href="styles.css">
</head>
<body>
  <h1>Data Mining & Machine Learning: Block 3 Exam Revision</h1>
  <div class="top-actions">
        <a class="home-btn" href="index.html" aria-label="Go to home">Home</a>
    </div>
  <p>
    <strong>Files covered:</strong>
   
  </p>
  <hr>

  <h2>I. Supervised vs Unsupervised Learning</h2>
  <h3>Supervised Learning</h3>
  <ul>
    <li>Instances: $x \in X$ (feature vectors)</li>
    <li>Unknown target function: $f: X \to Y$ (often binary: $\{-1, +1\}$)</li>
    <li>Input: Training examples $\{(x^{(1)}, y^{(1)}), \dots, (x^{(N)}, y^{(N)})\}$</li>
    <li>Output: Hypothesis $h \in H$ that best approximates $f$</li>
  </ul>

  <h3>Unsupervised Learning</h3>
  <ul>
    <li><strong>No class labels</strong> in the data</li>
    <li>Input: Training examples $\{x^{(1)}, \dots, x^{(N)}\}$</li>
    <li>Goal: Discover hidden structure, e.g., via <strong>clustering</strong></li>
  </ul>

  <hr>

  <h2>II. Clustering</h2>
  <h3>What is Clustering?</h3>
  <ul>
    <li>Automatically <strong>partition</strong> examples into groups of similar examples</li>
    <li>Useful for:
      <ul>
        <li>Organising data</li>
        <li>Understanding hidden structure</li>
        <li>Pre-processing for further analysis</li>
      </ul>
    </li>
    <li>Goal: Find groups so that data in one group are <strong>more similar to each other</strong> than to those in other groups</li>
  </ul>
  <h4>Examples of Clustering Applications</h4>
  <ul>
    <li>Image clustering</li>
    <li>Image segmentation</li>
    <li>Scientific paper topic clustering</li>
    <li>Gene identification</li>
    <li>Hub clusters in social networks</li>
  </ul>

  <hr>

  <h2>III. K-Means Clustering</h2>
  <h3>Overview</h3>
  <ul>
    <li>Input: Set $S$ of $n$ points in feature space, distance measure $d(x_i, x_j)$</li>
    <li>Output: Partition $\{S_1, S_2, ..., S_K\}$, where $K$ is the number of clusters</li>
    <li>K-Means is a <strong>hard clustering</strong> algorithm (each point belongs to exactly one cluster)</li>
    <li>Special case of the <strong>Expectation-Maximisation (EM)</strong> algorithm</li>
  </ul>

  <h3>Algorithm Steps</h3>
  <ol>
    <li><strong>Initialisation:</strong> Randomly initialise $K$ means ($\mu_k$)</li>
    <li><strong>E-step (Assignment):</strong> Assign each $x_n$ to the closest mean $\mu_k$<br>
      $z_n \leftarrow \arg\min_k |\mu_k - x_n|$
    </li>
    <li><strong>M-step (Re-estimation):</strong> Update each mean as the centroid of its assigned points<br>
      $\mu_k \leftarrow \frac{\sum_{n=1}^{N} z_{nk}x_n}{\sum_{n=1}^{N} z_{nk}}$
    </li>
    <li><strong>Termination:</strong> Repeat steps 2-3 until assignments or means stop changing</li>
  </ol>

  <h3>Cost Function</h3>
  <p>
    K-Means minimises the total squared distance between points and their assigned cluster means:
    <br>
    $L(z, \mu, D) = \sum_{n=1}^{N} \sum_{k=1}^{K} z_{nk}|x_n - \mu_k|^2$
  </p>

  <h3>Worked Example: M-step (Centroid Re-estimation)</h3>
  <ul>
    <li>Cluster $o$ contains 8 points. Feature 1 values: 0.7, 2.4, 3, 4.5, 5.2, 5.3, 6.2, 9.5</li>
    <li>Mean for F1: $(0.7 + 2.4 + 3 + 4.5 + 5.2 + 5.3 + 6.2 + 9.5)/8 = 4.6$</li>
    <li>If mean for F2 is 3.65, new centroid is $(4.6, 3.65)$</li>
  </ul>

  <h3>Worked Example: E-step (Assignment)</h3>
  <ul>
    <li>Point P1: (0.7, 5.1)</li>
    <li>Centroids: $\mu_o = (4.6, 3.65)$, $\mu_x = (5.2, 6.15)$</li>
    <li>Distance to $\mu_o$: $(4.6-0.7)^2 + (3.65-5.1)^2 = 17.37$</li>
    <li>Distance to $\mu_x$: $(5.2-0.7)^2 + (6.15-5.1)^2 = 21.35$</li>
    <li>Assign P1 to cluster $o$ (since 17.37 &lt; 21.35)</li>
  </ul>

  <hr>

  <h2>IV. Hard vs Soft Clustering</h2>
  <ul>
    <li><strong>Hard Clustering (K-Means):</strong> Each point assigned to exactly one cluster (binary indicator vector $z_n$)</li>
    <li><strong>Soft Clustering:</strong> Points can belong to multiple clusters with probabilities (e.g., Gaussian Mixture Models)</li>
  </ul>

  <hr>

  <h2>V. K-Means Limitations and Parameter Selection</h2>
  <h3>Convergence and Optimality</h3>
  <ul>
    <li>K-Means always converges in a finite number of iterations</li>
    <li>May converge to a <strong>local optimum</strong>, not necessarily the global optimum</li>
  </ul>

  <h3>Initialisation Sensitivity</h3>
  <ul>
    <li>Highly sensitive to initial means ($\mu_k$)</li>
    <li><strong>Furthest-first heuristic:</strong>
      <ol>
        <li>Pick a random example as first mean</li>
        <li>For each new mean, pick the point furthest from all previously chosen means</li>
      </ol>
    </li>
  </ul>

  <h3>Other Limitations</h3>
  <ul>
    <li>Sensitive to the <strong>scale</strong> of features</li>
    <li>Hard assignment can be a limitation for ambiguous points</li>
  </ul>

  <h3>Selecting the Number of Clusters ($K$)</h3>
  <ul>
    <li><strong>Elbow Criterion:</strong> Plot cost function vs $K$; choose $K$ at the "elbow" where cost plateaus</li>
    <li><strong>Silhouette Score:</strong> Measures cluster separation; ranges from -1 (bad) to +1 (good)</li>
  </ul>

  <hr>

  <h2>VI. Other Clustering Methods and Related Concepts</h2>
  <ul>
    <li><strong>Gaussian Mixture Models (GMM):</strong> Soft clustering alternative to K-Means</li>
    <li><strong>Clustering for Discretisation:</strong> Use clustering to convert numerical data to categorical bins</li>
    <li><strong>Dimensionality Reduction (e.g., PCA):</strong>
      <ul>
        <li>Reduces feature space dimensionality</li>
        <li>Removes noise, redundant features, compresses data, aids visualisation</li>
        <li>PCA maps data to lower dimension while maximising variance</li>
      </ul>
    </li>
  </ul>

  <hr>
  <h2>Exam Preparation Tips</h2>
  <ul>
    <li>Solve the <strong>Test on clustering (2.1)</strong></li>
    <li>Review the <strong>lab exercises</strong> that accompany the test</li>
    <li>Focus on understanding K-Means steps, cost function, and limitations</li>
    <li>Be able to work through assignment and centroid update examples</li>
    <li>Understand the difference between hard and soft clustering</li>
    <li>Know how to select $K$ and the impact of initialisation</li>
  </ul>
</body>
</html>
```