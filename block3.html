<!DOCTYPE html>
<html>
<head>
  <title>Data Mining & Machine Learning Exam Revision Guide</title>
  <meta charset="UTF-8">
  <link rel="stylesheet" href="styles.css">
</head>
<body>
  <h1>Data Mining & Machine Learning Exam Revision Guide</h1>
    <div class="top-actions">
        <a class="home-btn" href="index.html" aria-label="Go to home">Home</a>
    </div>
  <h2>Block 2: Probabilities and Bayesian Networks</h2>
  <p>
    Focus: Weeks 9–10. Worth up to <strong>14 points</strong> (Questions Q17–22).<br>
    <strong>Preparation:</strong> Solve Test 2.5.1 and 2.5.2 and review the associated lab exercises.
  </p>

  <h3>I. Core Probability Concepts and Formulas</h3>
  <h4>1. Unconditional Probabilities</h4>
  <ul>
    <li>Probability of a possible world <code>P(ω)</code> must satisfy <code>0 ≤ P(ω) ≤ 1</code>.</li>
    <li>Sum of probabilities for all possible worlds: <code>∑P(ω) = 1</code>.</li>
    <li>Probability of a proposition: sum of probabilities of worlds where it holds.</li>
    <li>Negation: <code>P(¬a) = 1 - P(a)</code>.</li>
    <li>Union: <code>P(a ∨ b) = P(a) + P(b) - P(a ∧ b)</code>.</li>
  </ul>

  <h4>2. Conditional Probability</h4>
  <ul>
    <li><strong>Definition:</strong> <code>P(a|b) = P(a ∧ b) / P(b)</code> (if <code>P(b) > 0</code>).</li>
    <li><strong>Independence:</strong> <code>P(a ∧ b) = P(a)P(b)</code> or <code>P(a|b) = P(a)</code>.</li>
  </ul>
  <p><strong>Example:</strong> Rolling two dice, probability of doubles given first die is 5:<br>
    <code>P(Doubles | Die1=5) = (1/36) / (1/6) = 1/6</code>
  </p>

  <h4>3. Bayes’ Rule and Bayesian Probability</h4>
  <ul>
    <li><strong>Bayes’ Rule:</strong> <code>P(a|b) = [P(b|a)P(a)] / P(b)</code></li>
    <li>
      <table border="1">
        <tr><th>Term</th><th>Name</th><th>Description</th></tr>
        <tr><td>P(a|b)</td><td>Posterior</td><td>Probability of a given b (updated belief)</td></tr>
        <tr><td>P(b|a)</td><td>Likelihood</td><td>Probability of b given a</td></tr>
        <tr><td>P(a)</td><td>Prior</td><td>Initial probability of a</td></tr>
        <tr><td>P(b)</td><td>Evidence</td><td>Total probability of b</td></tr>
      </table>
    </li>
  </ul>
  <p><strong>Worked Example:</strong> Customer Transactions<br>
    <ul>
      <li>P(A) = 0.4 (prior: buys)</li>
      <li>P(B) = 0.6 (evidence: streaming)</li>
      <li>P(B|A) = 0.2 (likelihood: streaming given buys)</li>
      <li>P(A|B) = (0.2 × 0.4) / 0.6 = 0.133 (posterior: buys given streaming)</li>
    </ul>
    <em>Belief is revised downward after observing streaming.</em>
  </p>

  <h3>II. Bayesian Networks (Bayes Nets)</h3>
  <ul>
    <li>Graphical models representing dependencies between variables (nodes = variables, arcs = dependencies).</li>
    <li>Structure: Directed Acyclic Graph (DAG).</li>
    <li>Conditional independence assumptions simplify calculations.</li>
    <li>Used in diagnostics, e-commerce, tailored ads, etc.</li>
  </ul>
  <p>
    <strong>Example (Medical Diagnostics):</strong><br>
    <ul>
      <li>P(meningitis) = 0.00002 (prior)</li>
      <li>P(stiff neck | meningitis) = 0.7 (likelihood)</li>
      <li>P(stiff neck) = 0.01 (evidence)</li>
      <li>P(meningitis | stiff neck) = (0.7 × 0.00002) / 0.01 = 0.0014 (posterior)</li>
    </ul>
    <em>Belief is revised upward after observing stiff neck.</em>
  </p>

  <h3>III. Probability Theory Fundamentals</h3>
  <ul>
    <li>Probabilistic assertions: about possible worlds (sample space).</li>
    <li>Logical assertions: true/false; probabilistic: how probable.</li>
    <li>Possible worlds are mutually exclusive and exhaustive.</li>
    <li>Probability model: function assigning P(ω) to each world.</li>
    <li>Unconditional (marginal) probabilities: belief without evidence.</li>
    <li>Conditional probabilities: belief updated with evidence.</li>
  </ul>

  <h3>IV. Chain Rule and Independence</h3>
  <ul>
    <li><strong>Chain Rule:</strong> Decompose joint probabilities:
      <br><code>P(s₁ ∧ s₂ ∧ ... ∧ sₖ) = P(s₁)P(s₂|s₁)P(s₃|s₁,s₂)...</code>
    </li>
    <li>Independence: <code>P(X,Y) = P(X)P(Y)</code> (unconditional), <code>P(X,Y|Z) = P(X|Z)P(Y|Z)</code> (conditional).</li>
    <li>Naive Bayes: all features independent given the class.</li>
  </ul>

  <h3>V. Naive Bayes</h3>
  <ul>
    <li>Assumes all features are independent given the class.</li>
    <li>
      <code>P(d|s₁,...,sₖ) = [P(d)P(s₁|d)...P(sₖ|d)] / P(s₁ ∧ ... ∧ sₖ)</code>
    </li>
    <li>Efficient for high-dimensional data, but assumption is "naive".</li>
  </ul>

  <h3>VI. Bayesian Networks: Structure and Inference</h3>
  <ul>
    <li>Nodes: random variables; arcs: direct influences (conditional probabilities).</li>
    <li>Each node has a conditional probability table (CPT) given its parents.</li>
    <li>Belief networks encode complex relationships and conditional independence.</li>
    <li>Inference: variable elimination algorithm (convert CPTs to factors, multiply, sum out irrelevant variables).</li>
    <li>For more: <a href="https://artint.info/3e/html/ArtInt3e.Ch9.html">AI: Foundations of Computational Agents, Ch. 9</a></li>
  </ul>

  <h3>VII. Decision Trees</h3>
  <ul>
    <li>Hierarchical, divide-and-conquer classifiers.</li>
    <li>Internal nodes: test features; leaves: assign class or probability.</li>
    <li>Classification: follow path from root to leaf using feature values.</li>
    <li>Work with nominal data; interpretable models.</li>
    <li>Recursive training: split on best feature (using Gini index, information gain, etc.) until pure or no features remain.</li>
    <li>Inference: traverse tree from root to leaf for new example.</li>
  </ul>

  <h4>Gini Index</h4>
  <ul>
    <li>Measures node impurity: <code>G = 1 - ∑pₖ²</code> (pₖ = proportion of class k).</li>
    <li>Gini = 0: pure node; higher Gini = more mixed.</li>
    <li>Used to select best split at each node.</li>
  </ul>

  <h4>Overfitting and Generalisation</h4>
  <ul>
    <li>Perfect fit on training data does not guarantee good generalisation.</li>
    <li>Prefer simpler trees (Occam’s razor) to avoid overfitting.</li>
    <li>Prevent overfitting by restricting splits, pruning (max depth), or using ensembles (random forests).</li>
    <li>Validate on separate data to assess generalisation.</li>
  </ul>

  <h3>VIII. Key Takeaways</h3>
  <ul>
    <li><strong>Bayesian Probability:</strong> Update beliefs with new evidence (prior → posterior).</li>
    <li><strong>Bayesian Networks:</strong> Model dependencies and conditional independence for efficient inference.</li>
    <li><strong>Decision Trees:</strong> Recursive, interpretable classifiers; risk of overfitting if too complex.</li>
    <li><strong>Gini Index:</strong> Used to measure node purity and select splits.</li>
    <li><strong>Generalisation:</strong> Simpler models often generalise better; validate on unseen data.</li>
  </ul>

  <h3>IX. Practice and Exam Tips</h3>
  <ul>
    <li>Practice with provided tests and lab exercises.</li>
    <li>Be able to compute priors, likelihoods, posteriors, and use Bayes’ rule.</li>
    <li>Understand and apply the chain rule, independence, and Naive Bayes assumptions.</li>
    <li>Be able to construct and interpret decision trees, calculate Gini index, and discuss overfitting.</li>
    <li>Review worked examples for Bayesian updating and decision tree splits.</li>
  </ul>

  <h3>X. Useful References</h3>
  <ul>
    <li><a href="file:///home/tor/Documents/My Vault/F20DL NotebookLM Block 3.md">Block 3 Revision Notes</a></li>
    <li><a href="file:///home/tor/Documents/My Vault/Bayes Rule and Bayesian Learning.md">Bayes Rule and Bayesian Learning</a></li>
    <li><a href="file:///home/tor/Documents/My Vault/Bayes Inference and Bayes Nets.md">Bayes Inference and Bayes Nets</a></li>
    <li><a href="file:///home/tor/Documents/My Vault/Decision Trees.md">Decision Trees</a></li>
  </ul>
</body>
</html>